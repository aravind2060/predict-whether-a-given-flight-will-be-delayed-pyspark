{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import logging\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s', datefmt='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session(app_name=\"Flight Delay Prediction\"):\n",
    "    \"\"\"\n",
    "    This function sets up a Spark session, which is the starting point for working with Spark.\n",
    "    Spark is a software tool that can help us analyze and process large amounts of data.\n",
    "    \"\"\"\n",
    "    logging.info(\"Creating Spark session...\")\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .getOrCreate()\n",
    "    logging.info(\"Spark session created.\")\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(spark, data_path):\n",
    "    \"\"\"\n",
    "    This function reads the Airlines dataset from the specified file path and returns it as a Spark DataFrame.\n",
    "    A Spark DataFrame is like a spreadsheet, where each row represents a flight and each column represents a piece of information about that flight.\n",
    "    \"\"\"\n",
    "    logging.info(\"Loading data from path: %s\", data_path)\n",
    "    df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "    logging.info(\"Data loaded.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    This function cleans the dataset by removing any duplicate flights and filling in any missing information.\n",
    "    Cleaning the data helps to ensure that the machine learning model we build later on will work as well as possible.\n",
    "    \"\"\"\n",
    "    logging.info(\"Cleaning the data...\")\n",
    "    df = df.dropDuplicates().na.drop()\n",
    "    logging.info(\"Data cleaning completed.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_variables(df):\n",
    "    \"\"\"\n",
    "    This function takes the dataset and converts the airline, origin airport, and destination airport information from text into numbers.\n",
    "    This is necessary because machine learning models work better with numbers than with text.\n",
    "    \"\"\"\n",
    "    logging.info(\"Encoding categorical variables...\")\n",
    "    indexers = [StringIndexer(inputCol=col, outputCol=col+\"_Indexed\").fit(df) for col in [\"Airline\", \"AirportFrom\", \"AirportTo\"]]\n",
    "    for indexer in indexers:\n",
    "        df = indexer.transform(df)\n",
    "    logging.info(\"Categorical variables encoded.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_features(df, input_cols, output_col):\n",
    "    \"\"\"\n",
    "    This function takes the different pieces of information about each flight (like the day of the week, departure time, and flight duration) and combines them into a single column called \"features\".\n",
    "    The machine learning model will use this \"features\" column to predict whether a flight will be delayed or not.\n",
    "    \"\"\"\n",
    "    logging.info(\"Assembling features...\")\n",
    "    assembler = VectorAssembler(inputCols=input_cols, outputCol=output_col)\n",
    "    df = assembler.transform(df)\n",
    "    logging.info(\"Features assembled.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, train_ratio=0.8, seed=1234):\n",
    "    \"\"\"\n",
    "    This function splits the dataset into two parts: a \"training\" set and a \"testing\" set.\n",
    "    The training set will be used to build the machine learning model, and the testing set will be used to check how well the model works.\n",
    "    \"\"\"\n",
    "    logging.info(\"Splitting data into training and testing sets...\")\n",
    "    train_data, test_data = df.randomSplit([train_ratio, 1-train_ratio], seed=seed)\n",
    "    logging.info(\"Data split completed.\")\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(train_data, test_data):\n",
    "    \"\"\"\n",
    "    Train and evaluate multiple machine learning models and return their performance metrics.\n",
    "    This function now includes evaluation of accuracy, precision, recall, and F1-score alongside AUC.\n",
    "    \"\"\"\n",
    "    logging.info(\"Training and evaluating models...\")\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(featuresCol=\"features\", labelCol=\"Delay\"),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"Delay\", maxBins=300),\n",
    "        \"Random Forest\": RandomForestClassifier(featuresCol=\"features\", labelCol=\"Delay\", maxBins=300),\n",
    "        \"Gradient Boosting\": GBTClassifier(featuresCol=\"features\", labelCol=\"Delay\", maxBins=300)\n",
    "    }\n",
    "\n",
    "    trained_models = {}\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        logging.info(f\"Training {name}...\")\n",
    "        trained_model = model.fit(train_data)\n",
    "        predictions = trained_model.transform(test_data)\n",
    "\n",
    "        logging.info(f\"Evaluating {name}...\")\n",
    "        # Evaluators for different metrics\n",
    "        evaluator_auc = BinaryClassificationEvaluator(labelCol=\"Delay\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "        evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"Delay\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "        evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"Delay\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "        evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"Delay\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "        evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"Delay\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "        # Calculating metrics\n",
    "        auc = evaluator_auc.evaluate(predictions)\n",
    "        accuracy = evaluator_acc.evaluate(predictions)\n",
    "        precision = evaluator_precision.evaluate(predictions)\n",
    "        recall = evaluator_recall.evaluate(predictions)\n",
    "        f1 = evaluator_f1.evaluate(predictions)\n",
    "\n",
    "        results[name] = {\n",
    "            \"AUC\": auc,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1-Score\": f1\n",
    "        }\n",
    "\n",
    "        trained_models[name] = trained_model\n",
    "        logging.info(f\"{name} - AUC: {auc}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}\")\n",
    "\n",
    "    return results, trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves(test_data, models):\n",
    "    \"\"\"\n",
    "    Plot ROC curves for each trained model to visualize their performance.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for name, model_info in models.items():\n",
    "        logging.info(f\"Plotting ROC curve for {name}...\")\n",
    "        predictions = model_info.transform(test_data)  # Ensure model_info is the model and not another dictionary or DataFrame\n",
    "        preds_and_labels = predictions.select(['probability', 'Delay']).toPandas()\n",
    "        preds_and_labels['probability'] = preds_and_labels['probability'].apply(lambda x: x[1])\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(preds_and_labels['Delay'], preds_and_labels['probability'])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves Comparison')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluation_metrics(results):\n",
    "    \"\"\"\n",
    "    This function plots a bar chart for the evaluation metrics of each model.\n",
    "    It visualizes accuracy, precision, recall, F1-score, and AUC.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Metrics to plot\n",
    "    metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC\"]\n",
    "    # Colors for different models\n",
    "    colors = ['blue', 'green', 'red', 'purple']\n",
    "\n",
    "    # Set up the matplotlib figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    n_groups = len(results)\n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.1\n",
    "    opacity = 0.8\n",
    "\n",
    "    # Loop through each metric and create a bar for it\n",
    "    for i, metric in enumerate(metrics):\n",
    "        values = [results[model][metric] for model in results]\n",
    "        plt.bar(index + i*bar_width, values, bar_width, alpha=opacity, color=colors[i % len(colors)], label=metric)\n",
    "\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Scores')\n",
    "    plt.title('Performance Metrics by Model')\n",
    "    plt.xticks(index + bar_width, list(results.keys()))\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(predictions, title='Confusion Matrix'):\n",
    "    \"\"\"\n",
    "    Plot a confusion matrix for the predictions made by the model.\n",
    "    \"\"\"\n",
    "    # Convert Spark DataFrame to Pandas DataFrame\n",
    "    y_true = predictions.select(\"Delay\").collect()\n",
    "    y_pred = predictions.select(\"prediction\").collect()\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(title)\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Create a Spark session\n",
    "    spark = create_spark_session()\n",
    "\n",
    "    # Load the Airlines dataset\n",
    "    data_path = \"/workspaces/predict-whether-a-given-flight-will-be-delayed-pyspark/airlines_dataset.csv\"  # Update this path as necessary\n",
    "    df = load_data(spark, data_path)\n",
    "\n",
    "    # Clean the data\n",
    "    df = clean_data(df)\n",
    "\n",
    "    # Encode the categorical variables\n",
    "    df = encode_categorical_variables(df)\n",
    "\n",
    "    # Assemble the features\n",
    "    df = assemble_features(df, input_cols=[\"DayOfWeek\", \"Time\", \"Length\", \"Airline_Indexed\", \"AirportFrom_Indexed\", \"AirportTo_Indexed\"], output_col=\"features\")\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_data, test_data = split_data(df)\n",
    "\n",
    "    # Train and evaluate models\n",
    "    results, trained_models = train_and_evaluate_models(train_data, test_data)\n",
    "\n",
    "    # Visualize ROC curves\n",
    "    plot_roc_curves(test_data, trained_models)  # Ensure trained_models is passed correctly\n",
    "\n",
    "    # Plot other performance metrics\n",
    "    plot_evaluation_metrics(results)\n",
    "    \n",
    "    # Plot confusion matrix for each model\n",
    "    for name, model in trained_models.items():\n",
    "        logging.info(f\"Plotting confusion matrix for {name}...\")\n",
    "        predictions = model.transform(test_data)\n",
    "        plt = plot_confusion_matrix(predictions, title=f'{name} Confusion Matrix')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
